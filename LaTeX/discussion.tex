% !TEX root = ./main.tex
\documentclass[main.tex]{subfiles}
\begin{document}
% Answer questions Why k = 10? What could our tests imply? Important of results.

As showed in both tests, the Brute-force was the slowest of all algorithms
tested, and could only handle numbers up to the magnitude of $10^{9}$. This
makes sense considering the time complexity, $\mathcal{O}(n)$. Since a modern
computer can do a little more than $10^{8}$ operations per second, a result of
$280$ seconds is expected. For the algorithm to verify $2^{61}-1$, it would take
around $10^{10}$ seconds, which is more than $300$ years. This demonstrates the
fact that efficient algorithms must be found. As $\mathcal{O}(n)$ grows
linearly, it does not take much for the runtime to exceed our lifetimes. \\

The Smart Brute-force, with its time complexity of $\mathcal{O}(\sqrt{n})$, was
considerably faster and completed $p=61$ in $100$ seconds, rather than $300$
years as in the case of the normal brute-force. However, that still doesn't
increase the algorithm's performance by much, since it exceeds the time-limit on
the next test, $p=107$. \\

On the other hand, Lucas-Lehmer could handle $2^{44,497}$, which is approximately
equal to $10^{13,394}$. Compared to the Brute-force algorithms, this is a
substantial step for the deterministic algorithms. Lucas-Lehmer demonstrates
how efficient an algorithm can become if it exploits properties of Mersenne primes. \\

WRITE ABOUT FERMAT AND MILLER-RABIN HERE \\

Overall, the algorithms tested correlate well to their respective Big O
notation. However, there is still a non-negligible variance. This is especially
shown in test (\ref{test1}) where all of the different sub-tests for each
algorithm yielded different numbers. The explanation for this phenomena is that
the performance of a computer is not constant. The change of CPU temperature,
clock speed and prioritisation of processes within the operating system, all
lead to change in performance. These factors are all beyond the control of the
user. In hindsight, it would be wise to have several sub-tests in (\ref{test2})
as-well, since it was clearly shown that performance fluctuates over time, and
that the tests done in (\ref{test2}) therefore are not reliable. This is
something that should be done if this study were to ever be re-done. \\

Even though the deterministic algorithms managed to verify numbers up to
$10^{13,394}$, it's nothing compared to the largest prime known to man,
$2^{82,589,933} - 1$. There are many factors as to why the results don't come
near the largest prime. One of the factors being that the algorithms chosen for
this study are both simple to understand and implement. This is done to satisfy
the purpose earlier introduced. The algorithms used in finding the largest
primes are \emph{extremely} complicated to understand and especially implement.
Other factors can be time and computers, which are huge limits for this study. \\

The most efficient algorithm which was tested by this study was the Lucas-Lehmer
algorithm, as shown in (\ref{test2}). It could handle $2^{44,497}-1$. This is
however nowhere near the current largest prime, $2^{82,589,933}-1$. There are
several different reasons for this. The current largest prime was not even found
using the Fermat primality test. The algorithm used to find this large number is
a more optimised version of the Lucas-Lehmer. This version is extremely
complicated and is far beyond the scope of this study, since it requires a ph.D.
to even understand. Even if the Fermat primality test were used to find this
large prime, we would not be able to recreate it. The laptop used in this study
is nothing compared to the GIMPS coalition of many computers, each
one of them more powerful than this laptop. \\

A future test regarding this subject could be improved by testing different
values for the constant $k$ and exploring how these different values would
affect the runtimes of the probabilistic algorithms. Another area of improvement
would be to increase the number of test cases in both (\ref{test1}) and
(\ref{test2}) in order to 1. draw conclusions regarding the variance and 2. draw
conclusions regarding the accuracy of the runtimes in test (\ref{test2}). Only
having one test case per algorithm and $p$ is like conducting a survey with one
test person, it is not accurate at all. \\

The constant $k$ was chosen to be $10$, which is a relatively arbitrary number.
Research on the internet did not yield a definite answer as to what value $k$
should have. Some algorithms had $k = 3$ while others had $k = 10$. The run
times of the probabilistic algorithms are proportional to this $k$ constant.
Therefore, testing different values of $k$ for each algorithm would be essential
to determine the actual efficiency of the probabilistic algorithms. Another
aspect of having an arbitrary $k$ is that the accuracy of the tests might be
impeded. No control was made to make sure that the algorithms actually returned
a true boolean value. This would also be a great area of improvement, as this
fact basically deems these tests as useless. \\

\end{document}
